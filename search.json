[{"title":"LaTex公式速查","url":"/2024/10/16/LaTex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/","content":"LaTex公式分为行内公式和单行公式，这里对公式内容和常见字符进行归纳整理：\n数学公式\n\n\n含义\n语法示例\n效果\n\n\n\n上角标\nx^1\n\n\n\n下角标\nx_1\n\n\n\n分数\n\\frac{x}{y}\n\n\n\n根号\n\\sqrt{x}\n\n\n\nn次方根\n\\sqrt[n]{x}\n\n\n\n积分\n\\int_{0}^{1}x\\mathrm{d}x\n\n\n\n极限\n\\lim_{x\\to\\infty}x\n\n\n\n累加\n\\sum_{n=1}^{100}n\n\n\n\n累和\n\\prod_{i=1}^{100}x_i\n\n\n\n单字符向量\n\\vec{x}\n\n\n\n多字符向右向量\n\\overrightarrow{xy}\n\n\n\n多字符向左向量\n\\overleftarrow{xy}\n\n\n\n重音\n\\hat{x}   \\bar{x}   \\tilde{x}\n    \n\n\n矩阵（有括号）\n\\begin{bmatrix}2&amp;4&amp;3 \\\\ 8&amp;6&amp;2\\end{bmatrix}\n\n\n\n矩阵（无括号）\n\\begin{matrix}2&amp;4&amp;3 \\\\ 8&amp;6&amp;2\\end{matrix}\n\n\n\n点乘\nx\\cdot y\n\n\n\n叉乘\nx\\times y\n\n\n\n除\nx\\div y\n\n\n\n约等于\nx\\approx y\n\n\n\n加减\nx\\pm 1\n\n\n\n小于等于\nx\\leq y\n\n\n\n大于等于\nx\\geq y\n\n\n\n常用逻辑符号\n\\forall \\infty \\emptyset \\exists \\nabla \\bot \\angle \\because \\therefore\n\n\n\n度\n\\degree\n\n\n\n希腊字符\n\n\n字符\n语法\n字符\n语法\n字符\n语法\n字符\n语法\n\n\n\n\n\\alpha\n\n\\kappa\n\n\\varsigma\n\n\\Lambda\n\n\n\n\\beta\n\n\\lambda\n\n\\tau\n\n\\Xi\n\n\n\n\\gamma\n\n\\mu\n\n\\upsilon\n\n\\Pi\n\n\n\n\\delta\n\n\\nu\n\n\\phi\n\n\\Sigma\n\n\n\n\\epsilon\n\n\\xi\n\n\\varphi\n\n\\Upsilon\n\n\n\n\\varepsilon\n\no\n\n\\chi\n\n\\Phi\n\n\n\n\\zeta\n\n\\pi\n\n\\psi\n\n\\Psi\n\n\n\n\\eta\n\n\\varpi\n\n\\omega\n\n\\Omega\n\n\n\n\\theta\n\n\\rho\n\n\\Gamma\n\n\n\n\n\n\\vartheta\n\n\\varrho\n\n\\Delta\n\n\n\n\n\n\\iota\n\n\\sigma\n\n\\Theta\n\n\n\n\n","tags":["速查表"]},{"title":"爬虫实战_2_7某9局豆瓣影评","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-2-7%E6%9F%909%E5%B1%80%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/","content":"上手实战前面已经介绍了爬虫的基础，这节我们对刚上映的某电影豆瓣影评进行实战。\n首先打开豆瓣，找到电影短评界面\n\n这个url就是我们要爬取的地址，直接上代码：\nimport pandas as pdimport requestsfrom bs4 import BeautifulSoup# 模拟浏览器头部信息，向豆瓣服务器发送消息headers=&#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) &#x27;                        &#x27;AppleWebKit/537.36 (KHTML, like Gecko) &#x27;                        &#x27;Chrome/121.0.0.0 Safari/537.36&#x27;&#125;url=&quot;https://movie.douban.com/subject/26747919/comments&quot;# 爬取页数pages=20comments=[]for page in range(pages):    # 获取HTML页面信息    r=requests.get(url=url+&#x27;?start=&#x27;+str(page*20),headers=headers) # 这里在遍历每页评论    r.encoding=&#x27;utf-8&#x27;    # 解析HTML信息并提取评论信息    soup=BeautifulSoup(r.text,&#x27;lxml&#x27;)    # 提取评论信息    comment_elements=soup.find_all(&#x27;span&#x27;,&#123;&#x27;class&#x27;:&#x27;short&#x27;&#125;)    for element in comment_elements:        comments.append(element.text.strip())# 创建一个包含评论内容和评论时间的数据框data=&#123;&#x27;评论内容&#x27;:comments&#125;df=pd.DataFrame(data)# 指定输出的Excel文件名output_filename=&quot;douban.xlsx&quot;# 将DataFrame保存为Excel文件df.to_excel(output_filename,index=False,engine=&#x27;openpyxl&#x27;)print(f&#x27;Saved: &#123;output_filename&#125;&#x27;)\n\n爬取结果如下：\n\n既然爬都爬了，这里就引入另一个有趣的方向 - - 自然语言处理之情感分析。\n情感分析这里先简单使用Snownlp以对情感分析有个基本的印象：\nfrom snownlp import SnowNLPimport pandas as pddf=pd.read_excel(&#x27;douban.xlsx&#x27;)# 对评论进行情感分析df[&#x27;情感分析&#x27;]=df[&#x27;评论内容&#x27;].apply(lambda x: SnowNLP(x).sentiments)# 统计各类情感def categorize_sentiment(score):    if score&gt;0.6:        return &#x27;正向&#x27;    elif score&lt;0.4:        return &#x27;负向&#x27;    else:        return &#x27;中性&#x27;# 创建新列进行分类df[&#x27;情感分类&#x27;]=df[&#x27;情感分析&#x27;].apply(categorize_sentiment)# 统计各类别数量和比例sentiment_counts=df[&#x27;情感分类&#x27;].value_counts()sentiment_proportion=df[&#x27;情感分类&#x27;].value_counts(normalize=True)*100# 打印结果print(df)print(&quot;\\n情感统计结果：&quot;)print(sentiment_counts)print(&quot;\\n情感比例：&quot;)print(sentiment_proportion)\n\n结果如下：\n\n很明显，存在很多不准确的情况。期待随着后面的学习，我们能够对其不断优化＼( ^▽^ )／\n","categories":["爬虫"],"tags":["情感分析","爬虫"]},{"title":"Win11_Kali子系统_图形化配置","url":"/2024/10/11/Win11-Kali%E5%AD%90%E7%B3%BB%E7%BB%9F-%E5%9B%BE%E5%BD%A2%E5%8C%96%E9%85%8D%E7%BD%AE/","content":"1. 开启服务，更新wsl2启用或关闭Windows功能 -&gt; 开启适用于Linux的Windows子系统，重启电脑\n\nWindows Powershell输入命令\n\nwsl –update    #更新到wsl2\nwsl –status    #检查一下\n\n2. 安装kali，换源配置工具\nwsl –install kali-linux    #安装kali-linux分发\n\n安装完成后设置用户名密码\n换源\n\nsudo vi &#x2F;etc&#x2F;apt&#x2F;sources.list\n\n如下为阿里云源\ndeb http://mirrors.aliyun.com/kali kali-rolling main non-free contribdeb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib\n\n 更新\n\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\nsudo apt-get clean\n\n 安装pip3\n\nsudo apt-get install python3-pip\n\n 安装kali工具，Github上的一个项目\n\ngit clone https://github.com/rikonaka/katoolin4china\ncd katoolin4china\nsudo pip3 install .\nsudo k4c\n2    #选项：安装工具\n0    #选项：安装所有工具\n\n安装过程跑完即可\n这里定位不到squid3，就决定要用的时候再去安装，这里略过\n\n3. 安装GUI + 更加方便的配置安装 Win-KeX\n\nsudo apt install -y kali-win-kex\n\n启动GUI\n\nkex    #启动kex\nkex stop    #停止kex\nkex –win -s    #窗口化\n\n 窗口模式启动后使用F8退出全屏\n\n这里强推一个终端工具 Windows Terminal，在Microsoft Store可以下载\n下面将KeX加入Terminal，由于我的Terminal在很久之前的Ubuntu子系统时就已经配置了，安装kali子系统后自动用的Terminal打开，如果是刚刚才安装的话可以自行配置一下\n\n\n","categories":["环境配置|报错记录"],"tags":["Kali"]},{"title":"Centos7安装MySQL并连接Django项目记录及报错解决","url":"/2024/10/11/Centos7%E5%AE%89%E8%A3%85MySQL%E5%B9%B6%E8%BF%9E%E6%8E%A5Django%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95%E5%8F%8A%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/","content":"1. 安装MySQL使用MySQL官方的Yum Repository\nwget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql-community-server\n\n此时断网了，意外下载中断，再次yum显示：\n直接kill 30656，如果不知道是哪个进程：\nps aux|grep yum\n\n找到对应yum进程再kill\n然后继续yum，出现报错：\n禁掉GPG验证检查，在之前的yum命令后面加上–nogpgcheck：\nyum -y install mysql-community-server --nogpgcheck\n\n至此安装完成。\n2. 配置MySQL启动MySQL服务\nsystemctl start mysqld.service\n\n检查MySQL服务状态\nsystemctl status mysqld.service\n\n在日志中查找初始MySQL密码\ngrep &quot;password&quot; /var/log/mysqld.log\n\n使用此密码登录\nmysql -uroot -p\n\n登录后使用此命令修改你的密码（passwd）处\nALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;passwd&#x27;;\n\n如果不那么需要安全性的话，可以修改数据库密码校验的规则\nset global validate_password_policy=0;\n\nset global validate_password_length=1;\n\n这样新密码会更容易通过\n3. 安装mysqlclientpip install mysqlclient\n\n4. 连接Django项目找到你的Django项目中settings.py中的数据库配置部分，将信息修改为刚刚配置的信息\n\n然后按照你写的NAME，创建一个数据库\ncreate database 数据库名;\n\nquit退出MySQL命令行\n最后在Django项目下运行命令\npython manage.py migrate\n\n哦，记得pip install Django和其他requirements。\n","categories":["环境配置|报错记录"],"tags":["Django"]},{"title":"爬虫实战_1_某大佬博客","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-1-%E6%9F%90%E5%A4%A7%E4%BD%AC%E5%8D%9A%E5%AE%A2/","content":"环境配置pip install requestspip install beautifulsoup4\n\nRequests: 让 HTTP 服务人类这里以P4tt0n大佬的博客https://p4tt0n.github.io/为例ψ(｀∇´)ψ\n先导入Requests库，其宗旨是”让HTTP服务人类”，使用它，可以很方便向网站发起请求，进行通信等：\n# 简单测试下import requests# 添加headers，有些请求要求加headers，非必要headers = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;r=requests.get(&#x27;https://p4tt0n.github.io/&#x27;,headers=headers)print(r.status_code)  # 返回请求状态码print(r.text)  # 返回页面代码print(r.json())  # 返回json\n\n成功返回数据\n\nRequests也可以发Post请求：\nheaders = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;# post请求，如果需要POST data，如需要登录进去data = &#123;&#x27;users&#x27;: &#x27;admin&#x27;, &#x27;password&#x27;: &#x27;admin&#x27;&#125;r = requests.post(&#x27;https://p4tt0n.github.io/&#x27;, data=data, headers=headers)\n\n若登录进去后，可以建立一个session对象保持会话，这样就不用每次都发送请求了：\n# 新建一个session对象sess = requests.session()# 先完成登录sess.post(&#x27;login url&#x27;, data=data, headers=headers)# 然后再在这个会话下去访问其他的网址sess.get(&#x27;other urls&#x27;)\n\nBeautifulsoup: 干了这碗美丽的汤在使用requests获取到整个页面的源码后，就需要beautifulsoup对数据进一步处理。\nBeautifulSoup是一个可以从HTML或XML文件中提取数据的Python库，翻译成中文是美丽的汤，与传统的tagsoup对应，希望改进传统不好的web格式，该名字出自《爱丽丝梦游仙境》的第十章《龙虾四组舞》：\n美丽的汤，如此浓郁和绿色在热腾腾的茶中等待着谁会不为这样的美味而弯腰呢？傍晚的汤 美丽的汤美丽的汤 美丽的汤 美丽的 sou-oop...美丽的汤 晚上的汤...好好喝的汤！美汤，谁在乎鱼？野味还是任何其他菜肴？谁不愿意为两个人付出一切呢？Pennyworth 唯有美丽的汤Pennyworth 唯有美丽的汤美丽的 sou-oop...美丽的汤 傍晚的汤 美丽，美丽的汤！\n\n这也就是BeautifulSoup官方文档中大量使用《爱丽丝梦游仙境》中文本作为例子的原因。下面我们就使用官方示例说明BeautifulSoup有多美丽：\nfrom bs4 import BeautifulSouphtml = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;# 创建一个名为soup的BeautifulSoup对象soup = BeautifulSoup(html, &#x27;lxml&#x27;)\n\n# 按照标准的缩进格式的结构输出print(soup.prettify())\n\n\n# 获取标题print(soup.title)\n\n&lt;title&gt;The Dormouse’s story&lt;&#x2F;title&gt;\n# 获取标题文本print(soup.title.text)\n\nThe Dormouse’s story\n# 获取所有文字内容print(soup.get_text())\n\nThe Dormouse&#x27;s storyThe Dormouse&#x27;s storyOnce upon a time there were three little sisters; and their names were,Lacie andTillie;and they lived at the bottom of a well....\n\n# 通过标签定位print(soup.find_all(&#x27;a&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]\n\n# 通过属性定位print(soup.find_all(attrs=&#123;&#x27;id&#x27;: &#x27;link1&#x27;&#125;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n# 标签 + 属性定位print(soup.find_all(&#x27;a&#x27;, id=&#x27;link1&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n博客爬虫实战这里我们试着爬取博客中的所有文章，先整理返回的html数据，观察特征：\nimport requestsfrom bs4 import BeautifulSoupr=requests.get(&#x27;https://p4tt0n.github.io/&#x27;)soup=BeautifulSoup(r.text, &#x27;lxml&#x27;)# 标准缩进输出print(soup.prettify())\n\n发现文章标题信息以这样存在：\n&lt;a class=&quot;article-title&quot; href=&quot;/2024/10/10/%E6%98%A5%E7%A7%8B%E4%BA%91%E9%95%9CInitial/&quot; title=&quot;春秋云镜Initial&quot;&gt;        春秋云镜Initial       &lt;/a&gt;\n\n指定a和class&#x3D;”article-title”，输出所有的title，也就是所有博客的标题：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;a&#x27;,class_=&quot;article-title&quot;):    print(link.get(&#x27;title&#x27;))\n\n成功爬取：\n春秋云镜Initial深入浅出.user.iniThinkPHP v5.0.24反序列化代码审计Nodejs沙箱逃逸命令执行深入浅出PHP反序列化Hello World\n\n同理，爬取所有文章：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;div&#x27;,class_=&quot;content&quot;):    print(link.get_text())\n\n成功爬取：\n\n","categories":["爬虫"],"tags":["爬虫"]},{"title":"爬虫实战-3-“朝俄韩关系”主题爬虫","url":"/2025/05/14/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-3-%E2%80%9C%E6%9C%9D%E4%BF%84%E9%9F%A9%E5%85%B3%E7%B3%BB%E2%80%9D%E4%B8%BB%E9%A2%98%E7%88%AC%E8%99%AB/","content":"一、爬取主题朝俄韩关系\n二、关键技术 \n图2.1 网络爬虫的系统结构[1]\n网页获取基本原理是模拟浏览器进行HTTP请求，爬虫客户端通过HTTP请求向web服务器发送请求，获取服务器端的响应后下载网页，完成爬虫系统的爬取工作（requests、urllib、Scrapy）。\n本次使用requests库模拟浏览器向服务器发送请求。\n\n图2.2 网页获取模块\n网页解析一个网页去噪的过程，主要是网页内容正文抽取，提取网页中的内容时需要分析页面的HTML结构，从中提取页面的有效信息。\n常见方法有通过BeautifulSoup对HTML结构解析、利用正则表达式抽取文本数据。\nBeautifulSoup主要是Xpath和CssSelector方法，针对网站的HTML标签可以提取出所需要的有效信息，可以选择tag、id、class等多种方式进行定位选择。\n但在复杂的页面结构中，需要采取正则表达式来提取有效信息。\n这里使用BeautifulSoup、urljoin结合对网页的分析来进行有效信息的提取。\n\n图2.3 网页解析模块\n数据存储爬虫抓取的数据一般有两种存储方式：本地保存，如CSV、Excel格式，或直接存储到数据库。\n一般写入数据库时采取爬一次数据就进行一次向量化清洗并入库，避免出现一些网络错误时已爬数据失效。\n这里保存在本地csv文件中，内容包括：url、title、similarity、content。\n\n图2.4 数据存储模块\n主题判别主要是判断爬取网页的主题相关性，大多被当做一个文本分类的问题来探索，目前结合网页中链接的锚文本、网页标签等来计算网页中URL与主题的相关度。\n常用的主题相似度判别算法有：\n\n向量空间模型：将文本处理转换为在向量空间上的向量运算，将每一篇文档表示为向量空间上的某一维度，通过计算向量在空间的相似度来衡量文档之间的相似度。\n语义相似度：主要问题是自然语言处理中语义理解，文本语义通常分析分词、统计词频。\n\n这里尝试使用TF-IDF+关键词匹配度+余弦相似度来判断相关度：\nTF-IDF（Term Frequency-Inverse Document Frequency, 词频-逆向文件频率）是一种用于评估一个词在一个文档中的重要程度的算法，考虑词在文档中的频率（TF）以及在所有文档中出现的频率（IDF），通过两者相乘得到TF-IDF值，TF-IDF权值越高，重要性越高。\n\n图2.5 TF计算公式（词在文档中出现次数&#x2F;文档中总次数）\n\n图2.6 IDF计算公式（log(总文档数&#x2F;包含词的文档数+1)）\n\n图2.7 TF-IDF计算公式（TF与IDF相乘）\n余弦相似度（Cosine Similarity）计算两个向量夹角的余弦值，用于衡量他们之间的相似度。\n\n图2.8 余弦相似度计算公式\n\n图2.9 主题判别模块\n网页搜索策略主要目的是使爬虫有次序、有目的地搜索，运用合理的搜索策略可以保证主题爬虫选择更合理的爬行路径。\n分为静态搜索策略和动态搜索策略，区别是有无事先确定搜索规则：\n静态：依照确定的规则进行搜索，不会改变。\n动态：根据相关度实时调整搜索路线，分为基于文本内容（Fish-Search、Shark-Search）和基于链接关系的搜索（PageRank、HITS、HillTop）。\n这里主要使用静态搜索策略，设置待访问、已访问链接列表和计数器，从主页出发，不断爬取链接，对于非文章类链接，深入爬取，直至文章。\n\n图2.10 网页搜索策略\n三、问题及解决问题：网站文章页面url的标识经过了编码，暂未发现可遍历规律：\n图3.1 爬取策略问题\n解决：\n从主页开始爬取，分为两种策略：\n\n爬取到分类链接时，即以’&#x2F;’结尾，则保留待继续爬取；\n爬取到文章链接时，即包含’&#x2F;c&#x2F;’，则直接开始解析。\n\n问题：使用TF-IDF+余弦相似度来判断相关度极低，无法爬取有效数据：解决：\n使用jieba分词，加入关键词频统计与TF-IDF+余弦相似度结合计算（目前效果不是很好，待改进）：\n\n图3.2 相似度计算问题\n问题：爬虫未设置停止条件：解决：加入计数器，设置最大爬取文章数和未访问链接及已访问链接。\n\n图3.3 爬虫结束问题\n四、爬取结果示例与思考 图4.1 爬虫结果示例\n从示例上看，虽然该文章完全符合预期主题，但是相关度仅是0.5，可能是相关度计算模块的TF-IDF方面并没有起到作用，而分词方面为1所致，需要后续对相关度计算研究学习。此外该次爬取的文章符合主题的并不多，大部分是少量相关，对于爬取技术方面还有所欠缺。\n整体来看，参考相关文献，对主题爬虫的过程基本实现，但若要达到实用且高效的程度，还需继续完善。\n五、参考文献[1] 潘晓英,陈柳,余慧敏,等.主题爬虫技术研究综述[J].计算机应用研究,2020,37(4):961-965,972.\n","categories":["爬虫"],"tags":["爬虫"]},{"title":"CI(Continuous Integration,持续集成) 与CD(Continuous Delivery,持续交付;Continuous Deploy,持续部署)","url":"/2025/05/14/CI-Continuous-Integration-%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90-%E4%B8%8ECD-Continuous-Delivery-%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98-Continuous-Deploy-%E6%8C%81%E7%BB%AD%E9%83%A8%E7%BD%B2/","content":"什么是CI&#x2F;CDCI&#x2F;CD 是持续集成和持续交付&#x2F;部署的缩写，旨在简化并加快软件开发生命周期。\n持续集成(CI)是指自动且频繁地将代码更改集成到共享源代码存储库中的做法。持续交付或持续部署(CD)是一个由两部分组成的过程，涉及代码更改的集成、测试和交付。持续交付不会自动部署到生产环境，持续部署则会自动将更新发布到生产环境。\n这些关联的事务通常被统称为“CI&#x2F;CD 管道”，由开发和运维团队以敏捷方式协同支持，采用的方法不是 DevOps 就是站点可靠性工程（SRE)。\n持续集成(CI)持续集成是一种面向开发人员的自动化流程，有助于更频繁地将代码更改合并回共享分支或“主干”。进行这些更新时，会触发测试步骤的自动执行，以确保合并代码更改的可靠性。\n如何工作在现在的开发模式中，一般的项目，协同开发是离不开的，这就涉及到多个开发人员编写处理自己负责的功能模块或者某些开发人员共同负责一个模块。但是，如果企业组织安排在一天内将所有分支源代码合并在一起（称为“合并日”），最终可能造成工作繁琐、耗时，而且需要手动完成。\n这是因为当某个开发人员单独对应用进行更改时，可能会与其他开发人员同时进行的其他更改发生冲突。如果每个开发人员都自定义自己的本地集成开发环境（IDE），而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。\nCI 可被视为一种解决方案，用于解决同时开发的应用因分支过多而可能相互冲突的问题。\n一旦提交请求合并到主分支，自动化构建工具运行不同级别的自动化测试（通常是单元测试和集成测试）来进行验证，以确保相应更改不会破坏应用。\n关键组件\n\n版本控制系统（Version Control System，VCS）例如Git，用于跟踪代码变更，协作开发，并确保团队成员之间的代码同步。\n自动化构建工具如Jenkins、Travis CI、CircleCI等，用于在每次代码提交时自动触发构建过程。\n单元测试框架例如JUnit（Java）、pytest（Python），用于确保代码的基本功能在集成后仍然有效。\n\n作用\n\n减少集成问题在传统的开发模式中，团队成员可能在各自的开发分支上独立工作，而在合并时可能会产生冲突和集成问题。CI通过持续集成代码，及时发现和解决这些问题，避免了集成地狱。\n提高代码质量CI 强调自动化测试，包括单元测试、集成测试等。每次代码变更都会触发这些测试，确保新代码不会破坏现有功能，并减少引入 bug 的可能性。这有助于提高整体代码质量。\n快速反馈CI 通过快速执行自动化构建和测试，提供了即时反馈。开发人员可以在提交代码后迅速得知其是否通过了构建和测试，帮助他们更快速地发现和修复问题。\n提高开发效率通过自动化构建、测试和部署，CI减少了手动操作的需求，提高了开发效率。开发人员可以专注于编写代码而不必花费过多时间在手动构建和测试上。\n自动化部署与持续交付（Continuous Delivery）和持续部署（Continuous Deployment）结合，CI 可以实现自动化部署。这意味着经过测试的代码变更可以自动部署到预定环境，实现快速且可靠的交付流程。\n团队协作CI 鼓励团队成员频繁集成代码，确保大家的工作在一个共享的代码库中协同进行。这促进了团队之间的协作和沟通，减少了因代码集成问题而导致的沟通障碍。\n降低风险通过频繁集成和自动测试，CI 减少了发布到生产环境时出现问题的可能性。提前发现和解决问题有助于降低风险，确保稳定的软件交付。\n\n持续交付(CD)持续交付是指自动执行 CI 中的构建、单元测试和集成测试后，自动将经过验证的代码发布到存储库。因此，要实现有效的持续交付流程，将 CI 内置到开发管道中显得非常重要。  \n在持续交付中，从合并代码更改到交付生产就绪型版本，每个阶段均涉及测试的自动化及代码发布的自动化。整个流程结束后，运维团队便可以迅速将应用部署到生产环境。  \n持续交付通常意味着对开发人员对应用所做的更改自动进行错误测试并将其上传到存储库（如 GitHub 或容器镜像仓库），然后由运维团队将其部署到实时的生产环境。它可以解决开发团队和业务团队之间的可见性和沟通不佳的问题。为此，持续交付的目的就是拥有一个可随时部署到生产环境的代码库，并确保以最少的工作量部署新代码。\n关键组件\n\n自动化部署工具例如Docker、Ansible、Kubernetes等，用于自动化地部署应用程序和其依赖。\n环境配置管理工具如Terraform，确保不同环境（开发、测试、生产）的一致性。\n持续监控和反馈使用工具如Prometheus、Grafana，确保在部署后能够监控应用程序的性能和稳定性。\n\n作用\n\n快速交付持续交付强调频繁、快速地将新的代码变更交付到生产环境。这使得团队能够更加迅速地响应用户需求，推出新功能或修复 bug。\n稳定交付通过自动化测试、自动化部署和验证流程，持续交付确保每次交付都是经过充分验证的，降低了引入错误的风险，提高了软件的稳定性。\n降低发布成本持续交付通过自动化流程降低了发布的人工成本。这意味着开发团队不再需要手动执行繁琐的部署步骤，减少了错误的可能性，提高了整体效率。\n支持持续改进持续交付是一个循环过程，通过不断收集用户反馈、监控系统性能和流程改进，团队能够不断优化持续交付流程，提高整体效率和质量。\n\n什么是持续部署？成熟的 CI&#x2F;CD 管道的最后一个阶段是持续部署。持续部署是持续交付的延伸，可以指自动将开发人员的更改从存储库发布到生产环境，以供客户使用。  \nCD 解决了运维团队因手动流程过多导致应用交付速度变慢的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。  \n实际上，持续部署意味着开发人员对云应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这使得持续接收和整合用户反馈变得更加容易。综上所述，所有这些相互关联的 CI&#x2F;CD 事务均能降低应用部署的风险，从而更轻松地以小块的形式发布对应用的更改，而不是一次性发布所有更改。  \n然而，由于在生产前的管道阶段没有人工关卡，因此，持续部署在很大程度上依赖于精心设计的测试自动化。这意味着持续部署可能需要大量的前期投入，因为需要编写自动化测试以适应 CI&#x2F;CD 管道中的各种测试和发布阶段。\nCI&#x2F;CD工具GitLab CIGitLab 是 CI&#x2F;CD 领域的一个新手玩家，但它已经在 Forrester Wave 持续集成工具中占据了领先地位。在这样一个竞争对手众多而水平又很高的领域，这是一项巨大的成就。是什么让 GitLab CI 如此了不起？    - 它使用 YAML 文件来描述整个管道。    - 它还有一个功能叫 Auto DevOps，使比较简单的项目可以自动构建内置了若干测试的管道。    - 使用 Herokuish 构建包来确定语言以及如何构建应用程序。有些语言还可以管理数据库，对于构建新的应用程序并在开发过程一开始就将其部署到生产环境中，这是一个很重要的功能。    - 提供到 Kubernetes 集群的原生集成，并使用多种部署方法的一种（如基于百分比的部署和蓝绿部署）将应用程序自动部署到 Kubernetes 集群中。    - 除了 CI 功能之外，GitLab 还提供了许多补充功能，比如自动把 Prometheus 和你的应用程序一起部署，实现运行监控；使用 GitLab 问题（Issues）、史诗（Epics）和里程碑（Milestones）进行项目组合和项目管理；管道内置了安全检查，提供跨多个项目的聚合结果；使用 WebIDE 在 GitLab 中编辑代码的能力，它甚至可以提供预览或执行管道的一部分，以获得更快的反馈。\nJenkinsJenkins 是 CI&#x2F;CD 领域中一款最早的、久负盛名的工具，是事实上的标准。对于大多数非开发人员来说，Jenkins 可能会是一个不小的负担，并且长期以来也一直是其管理员的负担。然而，这些都是他们想要解决的事项。\nJenkins 配置即代码（JCasC）应该有助于解决困扰管理员多年的复杂配置问题。和其他 CI&#x2F;CD 系统类似，它允许通过 YAML 文件实现 Jenkins 主节点的零接触配置。Jenkins Evergreen 的目标是通过提供基于不同用例的预定义 Jenkins 配置来简化这个过程。这些发行版应该比标准的 Jenkins 发行版更容易维护和升级。\nJenkins 2 引入了具有两种管道类型的原生管道功能。当你在做一些简单的事情时，这两种方法都不像 YAML 那么容易操作，但是它们非常适合处理更复杂的任务。\nJenkins X 是 Jenkins 的彻底转变，很可能是原生云 Jenkins 的实现（或者至少是大多数用户在使用原生云 Jenkins 时会看到的东西）。它将使用 JCasC 和 Evergreen，并在 Kubernetes 本地以最佳的方式使用它们。对于 Jenkins 来说，这是激动人心的时刻。\n参考文献深入理解CI&#x2F;CD：构建、测试和部署的完整流程_cicd自动化部署流程-CSDN博客什么是 CI&#x2F;CD？什么是CI&#x2F;CD，以及我所熟知的CI&#x2F;CD工具都是有哪些？_ci cd 工具是什么-CSDN博客\n","tags":["CI/CD"]}]