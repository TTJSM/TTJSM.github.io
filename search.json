[{"title":"LaTex公式速查","url":"/2024/10/16/LaTex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/","content":"LaTex公式分为行内公式和单行公式，这里对公式内容和常见字符进行归纳整理：\n数学公式\n\n\n含义\n语法示例\n效果\n\n\n\n上角标\nx^1\n\n\n\n下角标\nx_1\n\n\n\n分数\n\\frac{x}{y}\n\n\n\n根号\n\\sqrt{x}\n\n\n\nn次方根\n\\sqrt[n]{x}\n\n\n\n积分\n\\int_{0}^{1}x\\mathrm{d}x\n\n\n\n极限\n\\lim_{x\\to\\infty}x\n\n\n\n累加\n\\sum_{n=1}^{100}n\n\n\n\n累和\n\\prod_{i=1}^{100}x_i\n\n\n\n单字符向量\n\\vec{x}\n\n\n\n多字符向右向量\n\\overrightarrow{xy}\n\n\n\n多字符向左向量\n\\overleftarrow{xy}\n\n\n\n重音\n\\hat{x}   \\bar{x}   \\tilde{x}\n    \n\n\n矩阵（有括号）\n\\begin{bmatrix}2&amp;4&amp;3 \\\\ 8&amp;6&amp;2\\end{bmatrix}\n\n\n\n矩阵（无括号）\n\\begin{matrix}2&amp;4&amp;3 \\\\ 8&amp;6&amp;2\\end{matrix}\n\n\n\n点乘\nx\\cdot y\n\n\n\n叉乘\nx\\times y\n\n\n\n除\nx\\div y\n\n\n\n约等于\nx\\approx y\n\n\n\n加减\nx\\pm 1\n\n\n\n小于等于\nx\\leq y\n\n\n\n大于等于\nx\\geq y\n\n\n\n常用逻辑符号\n\\forall \\infty \\emptyset \\exists \\nabla \\bot \\angle \\because \\therefore\n\n\n\n度\n\\degree\n\n\n\n希腊字符\n\n\n字符\n语法\n字符\n语法\n字符\n语法\n字符\n语法\n\n\n\n\n\\alpha\n\n\\kappa\n\n\\varsigma\n\n\\Lambda\n\n\n\n\\beta\n\n\\lambda\n\n\\tau\n\n\\Xi\n\n\n\n\\gamma\n\n\\mu\n\n\\upsilon\n\n\\Pi\n\n\n\n\\delta\n\n\\nu\n\n\\phi\n\n\\Sigma\n\n\n\n\\epsilon\n\n\\xi\n\n\\varphi\n\n\\Upsilon\n\n\n\n\\varepsilon\n\no\n\n\\chi\n\n\\Phi\n\n\n\n\\zeta\n\n\\pi\n\n\\psi\n\n\\Psi\n\n\n\n\\eta\n\n\\varpi\n\n\\omega\n\n\\Omega\n\n\n\n\\theta\n\n\\rho\n\n\\Gamma\n\n\n\n\n\n\\vartheta\n\n\\varrho\n\n\\Delta\n\n\n\n\n\n\\iota\n\n\\sigma\n\n\\Theta\n\n\n\n\n","tags":["速查表"]},{"title":"爬虫实战_2_7某9局豆瓣影评","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-2-7%E6%9F%909%E5%B1%80%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/","content":"上手实战前面已经介绍了爬虫的基础，这节我们对刚上映的某电影豆瓣影评进行实战。\n首先打开豆瓣，找到电影短评界面\n\n这个url就是我们要爬取的地址，直接上代码：\nimport pandas as pdimport requestsfrom bs4 import BeautifulSoup# 模拟浏览器头部信息，向豆瓣服务器发送消息headers=&#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) &#x27;                        &#x27;AppleWebKit/537.36 (KHTML, like Gecko) &#x27;                        &#x27;Chrome/121.0.0.0 Safari/537.36&#x27;&#125;url=&quot;https://movie.douban.com/subject/26747919/comments&quot;# 爬取页数pages=20comments=[]for page in range(pages):    # 获取HTML页面信息    r=requests.get(url=url+&#x27;?start=&#x27;+str(page*20),headers=headers) # 这里在遍历每页评论    r.encoding=&#x27;utf-8&#x27;    # 解析HTML信息并提取评论信息    soup=BeautifulSoup(r.text,&#x27;lxml&#x27;)    # 提取评论信息    comment_elements=soup.find_all(&#x27;span&#x27;,&#123;&#x27;class&#x27;:&#x27;short&#x27;&#125;)    for element in comment_elements:        comments.append(element.text.strip())# 创建一个包含评论内容和评论时间的数据框data=&#123;&#x27;评论内容&#x27;:comments&#125;df=pd.DataFrame(data)# 指定输出的Excel文件名output_filename=&quot;douban.xlsx&quot;# 将DataFrame保存为Excel文件df.to_excel(output_filename,index=False,engine=&#x27;openpyxl&#x27;)print(f&#x27;Saved: &#123;output_filename&#125;&#x27;)\n\n爬取结果如下：\n\n既然爬都爬了，这里就引入另一个有趣的方向 - - 自然语言处理之情感分析。\n情感分析这里先简单使用Snownlp以对情感分析有个基本的印象：\nfrom snownlp import SnowNLPimport pandas as pddf=pd.read_excel(&#x27;douban.xlsx&#x27;)# 对评论进行情感分析df[&#x27;情感分析&#x27;]=df[&#x27;评论内容&#x27;].apply(lambda x: SnowNLP(x).sentiments)# 统计各类情感def categorize_sentiment(score):    if score&gt;0.6:        return &#x27;正向&#x27;    elif score&lt;0.4:        return &#x27;负向&#x27;    else:        return &#x27;中性&#x27;# 创建新列进行分类df[&#x27;情感分类&#x27;]=df[&#x27;情感分析&#x27;].apply(categorize_sentiment)# 统计各类别数量和比例sentiment_counts=df[&#x27;情感分类&#x27;].value_counts()sentiment_proportion=df[&#x27;情感分类&#x27;].value_counts(normalize=True)*100# 打印结果print(df)print(&quot;\\n情感统计结果：&quot;)print(sentiment_counts)print(&quot;\\n情感比例：&quot;)print(sentiment_proportion)\n\n结果如下：\n\n很明显，存在很多不准确的情况。期待随着后面的学习，我们能够对其不断优化＼( ^▽^ )／\n","categories":["爬虫"],"tags":["情感分析","爬虫"]},{"title":"Win11_Kali子系统_图形化配置","url":"/2024/10/11/Win11-Kali%E5%AD%90%E7%B3%BB%E7%BB%9F-%E5%9B%BE%E5%BD%A2%E5%8C%96%E9%85%8D%E7%BD%AE/","content":"1. 开启服务，更新wsl2启用或关闭Windows功能 -&gt; 开启适用于Linux的Windows子系统，重启电脑\n\nWindows Powershell输入命令\n\nwsl –update    #更新到wsl2\nwsl –status    #检查一下\n\n2. 安装kali，换源配置工具\nwsl –install kali-linux    #安装kali-linux分发\n\n安装完成后设置用户名密码\n换源\n\nsudo vi &#x2F;etc&#x2F;apt&#x2F;sources.list\n\n如下为阿里云源\ndeb http://mirrors.aliyun.com/kali kali-rolling main non-free contribdeb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib\n\n 更新\n\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\nsudo apt-get clean\n\n 安装pip3\n\nsudo apt-get install python3-pip\n\n 安装kali工具，Github上的一个项目\n\ngit clone https://github.com/rikonaka/katoolin4china\ncd katoolin4china\nsudo pip3 install .\nsudo k4c\n2    #选项：安装工具\n0    #选项：安装所有工具\n\n安装过程跑完即可\n这里定位不到squid3，就决定要用的时候再去安装，这里略过\n\n3. 安装GUI + 更加方便的配置安装 Win-KeX\n\nsudo apt install -y kali-win-kex\n\n启动GUI\n\nkex    #启动kex\nkex stop    #停止kex\nkex –win -s    #窗口化\n\n 窗口模式启动后使用F8退出全屏\n\n这里强推一个终端工具 Windows Terminal，在Microsoft Store可以下载\n下面将KeX加入Terminal，由于我的Terminal在很久之前的Ubuntu子系统时就已经配置了，安装kali子系统后自动用的Terminal打开，如果是刚刚才安装的话可以自行配置一下\n\n\n","categories":["环境配置|报错记录"],"tags":["Kali"]},{"title":"Centos7安装MySQL并连接Django项目记录及报错解决","url":"/2024/10/11/Centos7%E5%AE%89%E8%A3%85MySQL%E5%B9%B6%E8%BF%9E%E6%8E%A5Django%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95%E5%8F%8A%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/","content":"1. 安装MySQL使用MySQL官方的Yum Repository\nwget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql-community-server\n\n此时断网了，意外下载中断，再次yum显示：\n直接kill 30656，如果不知道是哪个进程：\nps aux|grep yum\n\n找到对应yum进程再kill\n然后继续yum，出现报错：\n禁掉GPG验证检查，在之前的yum命令后面加上–nogpgcheck：\nyum -y install mysql-community-server --nogpgcheck\n\n至此安装完成。\n2. 配置MySQL启动MySQL服务\nsystemctl start mysqld.service\n\n检查MySQL服务状态\nsystemctl status mysqld.service\n\n在日志中查找初始MySQL密码\ngrep &quot;password&quot; /var/log/mysqld.log\n\n使用此密码登录\nmysql -uroot -p\n\n登录后使用此命令修改你的密码（passwd）处\nALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;passwd&#x27;;\n\n如果不那么需要安全性的话，可以修改数据库密码校验的规则\nset global validate_password_policy=0;\n\nset global validate_password_length=1;\n\n这样新密码会更容易通过\n3. 安装mysqlclientpip install mysqlclient\n\n4. 连接Django项目找到你的Django项目中settings.py中的数据库配置部分，将信息修改为刚刚配置的信息\n\n然后按照你写的NAME，创建一个数据库\ncreate database 数据库名;\n\nquit退出MySQL命令行\n最后在Django项目下运行命令\npython manage.py migrate\n\n哦，记得pip install Django和其他requirements。\n","categories":["环境配置|报错记录"],"tags":["Django"]},{"title":"爬虫实战_1_某大佬博客","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-1-%E6%9F%90%E5%A4%A7%E4%BD%AC%E5%8D%9A%E5%AE%A2/","content":"环境配置pip install requestspip install beautifulsoup4\n\nRequests: 让 HTTP 服务人类这里以P4tt0n大佬的博客https://p4tt0n.github.io/为例ψ(｀∇´)ψ\n先导入Requests库，其宗旨是”让HTTP服务人类”，使用它，可以很方便向网站发起请求，进行通信等：\n# 简单测试下import requests# 添加headers，有些请求要求加headers，非必要headers = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;r=requests.get(&#x27;https://p4tt0n.github.io/&#x27;,headers=headers)print(r.status_code)  # 返回请求状态码print(r.text)  # 返回页面代码print(r.json())  # 返回json\n\n成功返回数据\n\nRequests也可以发Post请求：\nheaders = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;# post请求，如果需要POST data，如需要登录进去data = &#123;&#x27;users&#x27;: &#x27;admin&#x27;, &#x27;password&#x27;: &#x27;admin&#x27;&#125;r = requests.post(&#x27;https://p4tt0n.github.io/&#x27;, data=data, headers=headers)\n\n若登录进去后，可以建立一个session对象保持会话，这样就不用每次都发送请求了：\n# 新建一个session对象sess = requests.session()# 先完成登录sess.post(&#x27;login url&#x27;, data=data, headers=headers)# 然后再在这个会话下去访问其他的网址sess.get(&#x27;other urls&#x27;)\n\nBeautifulsoup: 干了这碗美丽的汤在使用requests获取到整个页面的源码后，就需要beautifulsoup对数据进一步处理。\nBeautifulSoup是一个可以从HTML或XML文件中提取数据的Python库，翻译成中文是美丽的汤，与传统的tagsoup对应，希望改进传统不好的web格式，该名字出自《爱丽丝梦游仙境》的第十章《龙虾四组舞》：\n美丽的汤，如此浓郁和绿色在热腾腾的茶中等待着谁会不为这样的美味而弯腰呢？傍晚的汤 美丽的汤美丽的汤 美丽的汤 美丽的 sou-oop...美丽的汤 晚上的汤...好好喝的汤！美汤，谁在乎鱼？野味还是任何其他菜肴？谁不愿意为两个人付出一切呢？Pennyworth 唯有美丽的汤Pennyworth 唯有美丽的汤美丽的 sou-oop...美丽的汤 傍晚的汤 美丽，美丽的汤！\n\n这也就是BeautifulSoup官方文档中大量使用《爱丽丝梦游仙境》中文本作为例子的原因。下面我们就使用官方示例说明BeautifulSoup有多美丽：\nfrom bs4 import BeautifulSouphtml = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;# 创建一个名为soup的BeautifulSoup对象soup = BeautifulSoup(html, &#x27;lxml&#x27;)\n\n# 按照标准的缩进格式的结构输出print(soup.prettify())\n\n\n# 获取标题print(soup.title)\n\n&lt;title&gt;The Dormouse’s story&lt;&#x2F;title&gt;\n# 获取标题文本print(soup.title.text)\n\nThe Dormouse’s story\n# 获取所有文字内容print(soup.get_text())\n\nThe Dormouse&#x27;s storyThe Dormouse&#x27;s storyOnce upon a time there were three little sisters; and their names were,Lacie andTillie;and they lived at the bottom of a well....\n\n# 通过标签定位print(soup.find_all(&#x27;a&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]\n\n# 通过属性定位print(soup.find_all(attrs=&#123;&#x27;id&#x27;: &#x27;link1&#x27;&#125;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n# 标签 + 属性定位print(soup.find_all(&#x27;a&#x27;, id=&#x27;link1&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n博客爬虫实战这里我们试着爬取博客中的所有文章，先整理返回的html数据，观察特征：\nimport requestsfrom bs4 import BeautifulSoupr=requests.get(&#x27;https://p4tt0n.github.io/&#x27;)soup=BeautifulSoup(r.text, &#x27;lxml&#x27;)# 标准缩进输出print(soup.prettify())\n\n发现文章标题信息以这样存在：\n&lt;a class=&quot;article-title&quot; href=&quot;/2024/10/10/%E6%98%A5%E7%A7%8B%E4%BA%91%E9%95%9CInitial/&quot; title=&quot;春秋云镜Initial&quot;&gt;        春秋云镜Initial       &lt;/a&gt;\n\n指定a和class&#x3D;”article-title”，输出所有的title，也就是所有博客的标题：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;a&#x27;,class_=&quot;article-title&quot;):    print(link.get(&#x27;title&#x27;))\n\n成功爬取：\n春秋云镜Initial深入浅出.user.iniThinkPHP v5.0.24反序列化代码审计Nodejs沙箱逃逸命令执行深入浅出PHP反序列化Hello World\n\n同理，爬取所有文章：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;div&#x27;,class_=&quot;content&quot;):    print(link.get_text())\n\n成功爬取：\n\n","categories":["爬虫"],"tags":["爬虫"]},{"title":"爬虫实战-3-“朝俄韩关系”主题爬虫","url":"/2025/05/14/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-3-%E2%80%9C%E6%9C%9D%E4%BF%84%E9%9F%A9%E5%85%B3%E7%B3%BB%E2%80%9D%E4%B8%BB%E9%A2%98%E7%88%AC%E8%99%AB/","content":"一、爬取主题朝俄韩关系\n二、关键技术 \n图2.1 网络爬虫的系统结构[1]\n网页获取基本原理是模拟浏览器进行HTTP请求，爬虫客户端通过HTTP请求向web服务器发送请求，获取服务器端的响应后下载网页，完成爬虫系统的爬取工作（requests、urllib、Scrapy）。\n本次使用requests库模拟浏览器向服务器发送请求。\n\n图2.2 网页获取模块\n网页解析一个网页去噪的过程，主要是网页内容正文抽取，提取网页中的内容时需要分析页面的HTML结构，从中提取页面的有效信息。\n常见方法有通过BeautifulSoup对HTML结构解析、利用正则表达式抽取文本数据。\nBeautifulSoup主要是Xpath和CssSelector方法，针对网站的HTML标签可以提取出所需要的有效信息，可以选择tag、id、class等多种方式进行定位选择。\n但在复杂的页面结构中，需要采取正则表达式来提取有效信息。\n这里使用BeautifulSoup、urljoin结合对网页的分析来进行有效信息的提取。\n\n图2.3 网页解析模块\n数据存储爬虫抓取的数据一般有两种存储方式：本地保存，如CSV、Excel格式，或直接存储到数据库。\n一般写入数据库时采取爬一次数据就进行一次向量化清洗并入库，避免出现一些网络错误时已爬数据失效。\n这里保存在本地csv文件中，内容包括：url、title、similarity、content。\n\n图2.4 数据存储模块\n主题判别主要是判断爬取网页的主题相关性，大多被当做一个文本分类的问题来探索，目前结合网页中链接的锚文本、网页标签等来计算网页中URL与主题的相关度。\n常用的主题相似度判别算法有：\n\n向量空间模型：将文本处理转换为在向量空间上的向量运算，将每一篇文档表示为向量空间上的某一维度，通过计算向量在空间的相似度来衡量文档之间的相似度。\n语义相似度：主要问题是自然语言处理中语义理解，文本语义通常分析分词、统计词频。\n\n这里尝试使用TF-IDF+关键词匹配度+余弦相似度来判断相关度：\nTF-IDF（Term Frequency-Inverse Document Frequency, 词频-逆向文件频率）是一种用于评估一个词在一个文档中的重要程度的算法，考虑词在文档中的频率（TF）以及在所有文档中出现的频率（IDF），通过两者相乘得到TF-IDF值，TF-IDF权值越高，重要性越高。\n\n图2.5 TF计算公式（词在文档中出现次数&#x2F;文档中总次数）\n\n图2.6 IDF计算公式（log(总文档数&#x2F;包含词的文档数+1)）\n\n图2.7 TF-IDF计算公式（TF与IDF相乘）\n余弦相似度（Cosine Similarity）计算两个向量夹角的余弦值，用于衡量他们之间的相似度。\n\n图2.8 余弦相似度计算公式\n\n图2.9 主题判别模块\n网页搜索策略主要目的是使爬虫有次序、有目的地搜索，运用合理的搜索策略可以保证主题爬虫选择更合理的爬行路径。\n分为静态搜索策略和动态搜索策略，区别是有无事先确定搜索规则：\n静态：依照确定的规则进行搜索，不会改变。\n动态：根据相关度实时调整搜索路线，分为基于文本内容（Fish-Search、Shark-Search）和基于链接关系的搜索（PageRank、HITS、HillTop）。\n这里主要使用静态搜索策略，设置待访问、已访问链接列表和计数器，从主页出发，不断爬取链接，对于非文章类链接，深入爬取，直至文章。\n\n图2.10 网页搜索策略\n三、问题及解决问题：网站文章页面url的标识经过了编码，暂未发现可遍历规律：\n图3.1 爬取策略问题\n解决：\n从主页开始爬取，分为两种策略：\n\n爬取到分类链接时，即以’&#x2F;’结尾，则保留待继续爬取；\n爬取到文章链接时，即包含’&#x2F;c&#x2F;’，则直接开始解析。\n\n问题：使用TF-IDF+余弦相似度来判断相关度极低，无法爬取有效数据：解决：\n使用jieba分词，加入关键词频统计与TF-IDF+余弦相似度结合计算（目前效果不是很好，待改进）：\n\n图3.2 相似度计算问题\n问题：爬虫未设置停止条件：解决：加入计数器，设置最大爬取文章数和未访问链接及已访问链接。\n\n图3.3 爬虫结束问题\n四、爬取结果示例与思考 图4.1 爬虫结果示例\n从示例上看，虽然该文章完全符合预期主题，但是相关度仅是0.5，可能是相关度计算模块的TF-IDF方面并没有起到作用，而分词方面为1所致，需要后续对相关度计算研究学习。此外该次爬取的文章符合主题的并不多，大部分是少量相关，对于爬取技术方面还有所欠缺。\n整体来看，参考相关文献，对主题爬虫的过程基本实现，但若要达到实用且高效的程度，还需继续完善。\n五、参考文献[1] 潘晓英,陈柳,余慧敏,等.主题爬虫技术研究综述[J].计算机应用研究,2020,37(4):961-965,972.\n","categories":["爬虫"],"tags":["爬虫"]}]