[{"title":"Centos7安装MySQL并连接Django项目记录及报错解决","url":"/2024/10/11/Centos7%E5%AE%89%E8%A3%85MySQL%E5%B9%B6%E8%BF%9E%E6%8E%A5Django%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95%E5%8F%8A%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/","content":"1. 安装MySQL使用MySQL官方的Yum Repository\nwget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql57-community-release-el7-10.noarch.rpm\n\nyum -y install mysql-community-server\n\n此时断网了，意外下载中断，再次yum显示：\n直接kill 30656，如果不知道是哪个进程：\nps aux|grep yum\n\n找到对应yum进程再kill\n然后继续yum，出现报错：\n禁掉GPG验证检查，在之前的yum命令后面加上–nogpgcheck：\nyum -y install mysql-community-server --nogpgcheck\n\n至此安装完成。\n2. 配置MySQL启动MySQL服务\nsystemctl start mysqld.service\n\n检查MySQL服务状态\nsystemctl status mysqld.service\n\n在日志中查找初始MySQL密码\ngrep &quot;password&quot; /var/log/mysqld.log\n\n使用此密码登录\nmysql -uroot -p\n\n登录后使用此命令修改你的密码（passwd）处\nALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;passwd&#x27;;\n\n如果不那么需要安全性的话，可以修改数据库密码校验的规则\nset global validate_password_policy=0;\n\nset global validate_password_length=1;\n\n这样新密码会更容易通过\n3. 安装mysqlclientpip install mysqlclient\n\n4. 连接Django项目找到你的Django项目中settings.py中的数据库配置部分，将信息修改为刚刚配置的信息\n\n然后按照你写的NAME，创建一个数据库\ncreate database 数据库名;\n\nquit退出MySQL命令行\n最后在Django项目下运行命令\npython manage.py migrate\n\n哦，记得pip install Django和其他requirements。\n","categories":["环境配置|报错记录"],"tags":["Django"]},{"title":"爬虫实战_2_7某9局豆瓣影评","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-2-7%E6%9F%909%E5%B1%80%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84/","content":"上手实战前面已经介绍了爬虫的基础，这节我们对刚上映的某电影豆瓣影评进行实战。\n首先打开豆瓣，找到电影短评界面\n\n这个url就是我们要爬取的地址，直接上代码：\nimport pandas as pdimport requestsfrom bs4 import BeautifulSoup# 模拟浏览器头部信息，向豆瓣服务器发送消息headers=&#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) &#x27;                        &#x27;AppleWebKit/537.36 (KHTML, like Gecko) &#x27;                        &#x27;Chrome/121.0.0.0 Safari/537.36&#x27;&#125;url=&quot;https://movie.douban.com/subject/26747919/comments&quot;# 爬取页数pages=20comments=[]for page in range(pages):    # 获取HTML页面信息    r=requests.get(url=url+&#x27;?start=&#x27;+str(page*20),headers=headers) # 这里在遍历每页评论    r.encoding=&#x27;utf-8&#x27;    # 解析HTML信息并提取评论信息    soup=BeautifulSoup(r.text,&#x27;lxml&#x27;)    # 提取评论信息    comment_elements=soup.find_all(&#x27;span&#x27;,&#123;&#x27;class&#x27;:&#x27;short&#x27;&#125;)    for element in comment_elements:        comments.append(element.text.strip())# 创建一个包含评论内容和评论时间的数据框data=&#123;&#x27;评论内容&#x27;:comments&#125;df=pd.DataFrame(data)# 指定输出的Excel文件名output_filename=&quot;douban.xlsx&quot;# 将DataFrame保存为Excel文件df.to_excel(output_filename,index=False,engine=&#x27;openpyxl&#x27;)print(f&#x27;Saved: &#123;output_filename&#125;&#x27;)\n\n爬取结果如下：\n\n既然爬都爬了，这里就引入另一个有趣的方向 - - 自然语言处理之情感分析。\n情感分析这里先简单使用Snownlp以对情感分析有个基本的印象：\nfrom snownlp import SnowNLPimport pandas as pddf=pd.read_excel(&#x27;douban.xlsx&#x27;)# 对评论进行情感分析df[&#x27;情感分析&#x27;]=df[&#x27;评论内容&#x27;].apply(lambda x: SnowNLP(x).sentiments)# 统计各类情感def categorize_sentiment(score):    if score&gt;0.6:        return &#x27;正向&#x27;    elif score&lt;0.4:        return &#x27;负向&#x27;    else:        return &#x27;中性&#x27;# 创建新列进行分类df[&#x27;情感分类&#x27;]=df[&#x27;情感分析&#x27;].apply(categorize_sentiment)# 统计各类别数量和比例sentiment_counts=df[&#x27;情感分类&#x27;].value_counts()sentiment_proportion=df[&#x27;情感分类&#x27;].value_counts(normalize=True)*100# 打印结果print(df)print(&quot;\\n情感统计结果：&quot;)print(sentiment_counts)print(&quot;\\n情感比例：&quot;)print(sentiment_proportion)\n\n结果如下：\n\n很明显，存在很多不准确的情况。期待随着后面的学习，我们能够对其不断优化＼( ^▽^ )／\n","categories":["爬虫"],"tags":["情感分析","爬虫"]},{"title":"Win11_Kali子系统_图形化配置","url":"/2024/10/11/Win11-Kali%E5%AD%90%E7%B3%BB%E7%BB%9F-%E5%9B%BE%E5%BD%A2%E5%8C%96%E9%85%8D%E7%BD%AE/","content":"1. 开启服务，更新wsl2启用或关闭Windows功能 -&gt; 开启适用于Linux的Windows子系统，重启电脑\n\nWindows Powershell输入命令\n\nwsl –update    #更新到wsl2\nwsl –status    #检查一下\n\n2. 安装kali，换源配置工具\nwsl –install kali-linux    #安装kali-linux分发\n\n安装完成后设置用户名密码\n换源\n\nsudo vi &#x2F;etc&#x2F;apt&#x2F;sources.list\n\n如下为阿里云源\ndeb http://mirrors.aliyun.com/kali kali-rolling main non-free contribdeb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib\n\n 更新\n\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\nsudo apt-get clean\n\n 安装pip3\n\nsudo apt-get install python3-pip\n\n 安装kali工具，Github上的一个项目\n\ngit clone https://github.com/rikonaka/katoolin4china\ncd katoolin4china\nsudo pip3 install .\nsudo k4c\n2    #选项：安装工具\n0    #选项：安装所有工具\n\n安装过程跑完即可\n这里定位不到squid3，就决定要用的时候再去安装，这里略过\n\n3. 安装GUI + 更加方便的配置安装 Win-KeX\n\nsudo apt install -y kali-win-kex\n\n启动GUI\n\nkex    #启动kex\nkex stop    #停止kex\nkex –win -s    #窗口化\n\n 窗口模式启动后使用F8退出全屏\n\n这里强推一个终端工具 Windows Terminal，在Microsoft Store可以下载\n下面将KeX加入Terminal，由于我的Terminal在很久之前的Ubuntu子系统时就已经配置了，安装kali子系统后自动用的Terminal打开，如果是刚刚才安装的话可以自行配置一下\n\n\n","categories":["环境配置|报错记录"],"tags":["Kali"]},{"title":"爬虫实战_1_某大佬博客","url":"/2024/10/11/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98-1-%E6%9F%90%E5%A4%A7%E4%BD%AC%E5%8D%9A%E5%AE%A2/","content":"环境配置pip install requestspip install beautifulsoup4\n\nRequests: 让 HTTP 服务人类这里以P4tt0n大佬的博客https://p4tt0n.github.io/为例ψ(｀∇´)ψ\n先导入Requests库，其宗旨是”让HTTP服务人类”，使用它，可以很方便向网站发起请求，进行通信等：\n# 简单测试下import requests# 添加headers，有些请求要求加headers，非必要headers = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;r=requests.get(&#x27;https://p4tt0n.github.io/&#x27;,headers=headers)print(r.status_code)  # 返回请求状态码print(r.text)  # 返回页面代码print(r.json())  # 返回json\n\n成功返回数据\n\nRequests也可以发Post请求：\nheaders = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE&#x27;&#125;# post请求，如果需要POST data，如需要登录进去data = &#123;&#x27;users&#x27;: &#x27;admin&#x27;, &#x27;password&#x27;: &#x27;admin&#x27;&#125;r = requests.post(&#x27;https://p4tt0n.github.io/&#x27;, data=data, headers=headers)\n\n若登录进去后，可以建立一个session对象保持会话，这样就不用每次都发送请求了：\n# 新建一个session对象sess = requests.session()# 先完成登录sess.post(&#x27;login url&#x27;, data=data, headers=headers)# 然后再在这个会话下去访问其他的网址sess.get(&#x27;other urls&#x27;)\n\nBeautifulsoup: 干了这碗美丽的汤在使用requests获取到整个页面的源码后，就需要beautifulsoup对数据进一步处理。\nBeautifulSoup是一个可以从HTML或XML文件中提取数据的Python库，翻译成中文是美丽的汤，与传统的tagsoup对应，希望改进传统不好的web格式，该名字出自《爱丽丝梦游仙境》的第十章《龙虾四组舞》：\n美丽的汤，如此浓郁和绿色在热腾腾的茶中等待着谁会不为这样的美味而弯腰呢？傍晚的汤 美丽的汤美丽的汤 美丽的汤 美丽的 sou-oop...美丽的汤 晚上的汤...好好喝的汤！美汤，谁在乎鱼？野味还是任何其他菜肴？谁不愿意为两个人付出一切呢？Pennyworth 唯有美丽的汤Pennyworth 唯有美丽的汤美丽的 sou-oop...美丽的汤 傍晚的汤 美丽，美丽的汤！\n\n这也就是BeautifulSoup官方文档中大量使用《爱丽丝梦游仙境》中文本作为例子的原因。下面我们就使用官方示例说明BeautifulSoup有多美丽：\nfrom bs4 import BeautifulSouphtml = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;# 创建一个名为soup的BeautifulSoup对象soup = BeautifulSoup(html, &#x27;lxml&#x27;)\n\n# 按照标准的缩进格式的结构输出print(soup.prettify())\n\n\n# 获取标题print(soup.title)\n\n&lt;title&gt;The Dormouse’s story&lt;&#x2F;title&gt;\n# 获取标题文本print(soup.title.text)\n\nThe Dormouse’s story\n# 获取所有文字内容print(soup.get_text())\n\nThe Dormouse&#x27;s storyThe Dormouse&#x27;s storyOnce upon a time there were three little sisters; and their names were,Lacie andTillie;and they lived at the bottom of a well....\n\n# 通过标签定位print(soup.find_all(&#x27;a&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]\n\n# 通过属性定位print(soup.find_all(attrs=&#123;&#x27;id&#x27;: &#x27;link1&#x27;&#125;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n# 标签 + 属性定位print(soup.find_all(&#x27;a&#x27;, id=&#x27;link1&#x27;))\n\n[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;]\n\n博客爬虫实战这里我们试着爬取博客中的所有文章，先整理返回的html数据，观察特征：\nimport requestsfrom bs4 import BeautifulSoupr=requests.get(&#x27;https://p4tt0n.github.io/&#x27;)soup=BeautifulSoup(r.text, &#x27;lxml&#x27;)# 标准缩进输出print(soup.prettify())\n\n发现文章标题信息以这样存在：\n&lt;a class=&quot;article-title&quot; href=&quot;/2024/10/10/%E6%98%A5%E7%A7%8B%E4%BA%91%E9%95%9CInitial/&quot; title=&quot;春秋云镜Initial&quot;&gt;        春秋云镜Initial       &lt;/a&gt;\n\n指定a和class&#x3D;”article-title”，输出所有的title，也就是所有博客的标题：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;a&#x27;,class_=&quot;article-title&quot;):    print(link.get(&#x27;title&#x27;))\n\n成功爬取：\n春秋云镜Initial深入浅出.user.iniThinkPHP v5.0.24反序列化代码审计Nodejs沙箱逃逸命令执行深入浅出PHP反序列化Hello World\n\n同理，爬取所有文章：\n#‘class’在python中是保留字，所以使用时需加‘下划线_’for link in soup.find_all(&#x27;div&#x27;,class_=&quot;content&quot;):    print(link.get_text())\n\n成功爬取：\n\n","categories":["爬虫"],"tags":["爬虫"]},{"title":"LaTex公式速查","url":"/2024/10/16/LaTex%E5%85%AC%E5%BC%8F%E9%80%9F%E6%9F%A5/","content":"LaTex公式分为行内公式（公式内容）和单行公式（公式内容），这里对公式内容和常见字符进行归纳整理：\n数学公式\n\n\n含义\n语法示例\n效果\n\n\n\n上角标\nx^1\n\n\n\n下角标\nx_1\n\n\n\n分数\n\\frac{x}{y}\n\n\n\n根号\n\\sqrt{x}\n\n\n\nn次方根\n\\sqrt[n]{x}\n\n\n\n积分\n\\int_{0}^{1}x\\mathrm{d}x\n\n\n\n极限\n\\lim_{x\\to\\infty}x\n\n\n\n累加\n\\sum_{n=1}^{100}n\n\n\n\n累和\n\\prod_{i=1}^{100}x_i\n\n\n\n单字符向量\n\\vec{x}\n\n\n\n多字符向右向量\n\\overrightarrow{xy}\n\n\n\n多字符向左向量\n\\overleftarrow{xy}\n\n\n\n重音\n\\hat{x}   \\bar{x}   \\tilde{x}\n    \n\n\n矩阵(有括号)\n\n\n\n\n矩阵（无括号）\n\n\n\n\n点乘\nx\\cdot y\n\n\n\n叉乘\nx\\times y\n\n\n\n除\nx\\div y\n\n\n\n约等于\nx\\approx y\n\n\n\n加减\nx\\pm 1\n\n\n\n小于等于\nx\\leq y\n\n\n\n大于等于\nx\\geq y\n\n\n\n常用逻辑符号\n\\forall \\infty \\emptyset \\exists \\nabla \\bot \\angle \\because \\therefore\n\n\n\n度\n\\degree\n\n\n\n希腊字符\n\n\n字符\n语法\n字符\n语法\n字符\n语法\n字符\n语法\n\n\n\n\n\\alpha\n\n\\kappa\n\n\\varsigma\n\n\\Lambda\n\n\n\n\\beta\n\n\\lambda\n\n\\tau\n\n\\Xi\n\n\n\n\\gamma\n\n\\mu\n\n\\upsilon\n\n\\Pi\n\n\n\n\\delta\n\n\\nu\n\n\\phi\n\n\\Sigma\n\n\n\n\\epsilon\n\n\\xi\n\n\\varphi\n\n\\Upsilon\n\n\n\n\\varepsilon\n\no\n\n\\chi\n\n\\Phi\n\n\n\n\\zeta\n\n\\pi\n\n\\psi\n\n\\Psi\n\n\n\n\\eta\n\n\\varpi\n\n$\\omega\n\n\\Omega\n\n\n\n\\theta\n\n\\rho\n\n\\Gamma\n\n\n\n\n\n\\vartheta\n\n\\varrho\n\n\\Delta\n\n\n\n\n\n\\iota\n\n\\sigma\n\n\\Theta\n\n\n\n\n","tags":["速查表"]}]